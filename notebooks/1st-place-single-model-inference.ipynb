{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9e5f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T11:36:47.408659Z",
     "iopub.status.busy": "2023-10-16T11:36:47.408309Z",
     "iopub.status.idle": "2023-10-16T11:36:54.253737Z",
     "shell.execute_reply": "2023-10-16T11:36:54.252847Z"
    },
    "papermill": {
     "duration": 6.852543,
     "end_time": "2023-10-16T11:36:54.255873",
     "exception": false,
     "start_time": "2023-10-16T11:36:47.403330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64318c7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T11:36:54.264793Z",
     "iopub.status.busy": "2023-10-16T11:36:54.263643Z",
     "iopub.status.idle": "2023-10-16T11:37:31.034086Z",
     "shell.execute_reply": "2023-10-16T11:37:31.032899Z"
    },
    "papermill": {
     "duration": 36.776783,
     "end_time": "2023-10-16T11:37:31.036486",
     "exception": false,
     "start_time": "2023-10-16T11:36:54.259703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/sci-llm-pip-v2/sentence-transformers-2.2.2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d27ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T11:37:31.046581Z",
     "iopub.status.busy": "2023-10-16T11:37:31.046276Z",
     "iopub.status.idle": "2023-10-16T11:37:31.146365Z",
     "shell.execute_reply": "2023-10-16T11:37:31.145437Z"
    },
    "papermill": {
     "duration": 0.107402,
     "end_time": "2023-10-16T11:37:31.148375",
     "exception": false,
     "start_time": "2023-10-16T11:37:31.040973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"/kaggle/input/kaggle-llm-science-exam\")\n",
    "\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    test = pd.read_csv(data_path / \"test.csv\")\n",
    "    CALC_SCORE = False\n",
    "else:\n",
    "    test = pd.read_csv(data_path / \"train.csv\")\n",
    "    CALC_SCORE = True\n",
    "\n",
    "test.head()\n",
    "test.to_parquet(\"test_raw.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7212b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T11:37:31.158362Z",
     "iopub.status.busy": "2023-10-16T11:37:31.158099Z",
     "iopub.status.idle": "2023-10-16T11:37:31.167832Z",
     "shell.execute_reply": "2023-10-16T11:37:31.166669Z"
    },
    "papermill": {
     "duration": 0.017037,
     "end_time": "2023-10-16T11:37:31.169719",
     "exception": false,
     "start_time": "2023-10-16T11:37:31.152682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile get_topk.py\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import argparse\n",
    "\n",
    "\n",
    "def cos_similarity_matrix(a: torch.Tensor, b: torch.Tensor):\n",
    "    \"\"\"Calculates cosine similarities between tensor a and b.\"\"\"\n",
    "\n",
    "    sim_mt = torch.mm(a, b.transpose(0, 1))\n",
    "    return sim_mt\n",
    "\n",
    "\n",
    "def get_topk(embeddings_from, embeddings_to, topk=1000, bs=512):\n",
    "    chunk = bs\n",
    "    embeddings_chunks = embeddings_from.split(chunk)\n",
    "\n",
    "    vals = []\n",
    "    inds = []\n",
    "    for idx in range(len(embeddings_chunks)):\n",
    "        cos_sim_chunk = cos_similarity_matrix(\n",
    "            embeddings_chunks[idx].to(embeddings_to.device).half(), embeddings_to\n",
    "        ).float()\n",
    "\n",
    "        cos_sim_chunk = torch.nan_to_num(cos_sim_chunk, nan=0.0)\n",
    "\n",
    "        topk = min(topk, cos_sim_chunk.size(1))\n",
    "        vals_chunk, inds_chunk = torch.topk(cos_sim_chunk, k=topk, dim=1)\n",
    "        vals.append(vals_chunk[:, :].detach().cpu())\n",
    "        inds.append(inds_chunk[:, :].detach().cpu())\n",
    "\n",
    "        del vals_chunk\n",
    "        del inds_chunk\n",
    "        del cos_sim_chunk\n",
    "\n",
    "    vals = torch.cat(vals).detach().cpu()\n",
    "    inds = torch.cat(inds).detach().cpu()\n",
    "\n",
    "    return inds, vals\n",
    "\n",
    "\n",
    "def insert_value_at(tensor, value, position):\n",
    "    # Ensure the position is valid\n",
    "    if position < 0 or position >= len(tensor):\n",
    "        raise ValueError(\"Position should be between 0 and tensor length - 1.\")\n",
    "\n",
    "    # Slice the tensor into two parts\n",
    "    left = tensor[:position]\n",
    "    right = tensor[position:]\n",
    "\n",
    "    # Create a tensor for the value to be inserted\n",
    "    value_tensor = torch.tensor([value], dtype=tensor.dtype)\n",
    "\n",
    "    # Concatenate the tensors together and slice to the original length\n",
    "    result = torch.cat([left, value_tensor, right])[:-1]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def insert_value_at_list(lst, value, position):\n",
    "    # Ensure the position is valid\n",
    "    if position < 0 or position >= len(lst):\n",
    "        raise ValueError(\"Position should be between 0 and list length - 1.\")\n",
    "\n",
    "    # Insert value at the specified position\n",
    "    lst.insert(position, value)\n",
    "\n",
    "    # Remove the last value to maintain original length\n",
    "    lst.pop()\n",
    "\n",
    "    return lst\n",
    "\n",
    "\n",
    "def remove_consecutive_duplicates(input_list):\n",
    "    if not input_list:\n",
    "        return [\" \"] * args.topk\n",
    "\n",
    "    new_list = [input_list[0]]\n",
    "    for i in range(1, len(input_list)):\n",
    "        if input_list[i] != input_list[i - 1]:\n",
    "            new_list.append(input_list[i])\n",
    "\n",
    "    # Append empty strings if new_list length is less than 5\n",
    "    while len(new_list) < args.topk:\n",
    "        new_list.append(\" \")\n",
    "\n",
    "    return new_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--wiki\", type=str, required=True)\n",
    "    ap.add_argument(\"--model_name\", type=str, required=True)\n",
    "    ap.add_argument(\"--test_file\", type=str, required=True)\n",
    "    ap.add_argument(\"--topk\", type=int, required=True)\n",
    "    ap.add_argument(\"--ind\", type=int, required=True)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    if args.topk == 10:\n",
    "        TOP_K = 20\n",
    "    else:\n",
    "        TOP_K = 10\n",
    "\n",
    "    data_path = Path(\"/kaggle/input/kaggle-llm-science-exam\")\n",
    "\n",
    "    if args.wiki == \"cirrus\":\n",
    "        files_all = sorted(list(glob(\"/kaggle/input/cirruswiki-titles/*.parquet\")))\n",
    "    elif args.wiki == \"new\":\n",
    "        files_all = sorted(list(glob(\"/kaggle/input/newwiki-titles/*.parquet\")))\n",
    "\n",
    "    if \"e5-large\" in args.model_name:\n",
    "        files_np = sorted(\n",
    "            list(glob(\"/kaggle/input/enwiki-cirrus-20230701-e5-large-part*/*.npy\"))\n",
    "        )\n",
    "    elif \"gte-large\" in args.model_name:\n",
    "        files_np = sorted(\n",
    "            list(glob(\"/kaggle/input/wiki31m-gte-large-title-p*/*.npy\"))\n",
    "        )\n",
    "\n",
    "    files_all = [(x, y) for x, y in zip(files_all, files_np)]\n",
    "    files = [files_all[: len(files_all) // 2], files_all[len(files_all) // 2 :]]\n",
    "\n",
    "    if \"e5-large\" in args.model_name:\n",
    "        model = SentenceTransformer(\"/kaggle/input/intfloat-e5-large-v2\").to(\"cuda:0\")\n",
    "    elif \"gte-large\" in args.model_name:\n",
    "        model = SentenceTransformer(\"/kaggle/input/thenlper-gte-large\").to(\"cuda:0\")\n",
    "\n",
    "    test = pd.read_parquet(\"test_raw.pq\")\n",
    "\n",
    "    embs = []\n",
    "    for idx, row in test.iterrows():\n",
    "        if \"e5\" in args.model_name:\n",
    "            sentences = [\n",
    "                \"query: \"\n",
    "                + row.prompt\n",
    "                + \" \"\n",
    "                + row.A\n",
    "                + \" \"\n",
    "                + row.B\n",
    "                + \" \"\n",
    "                + row.C\n",
    "                + \" \"\n",
    "                + row.D\n",
    "                + \" \"\n",
    "                + row.E\n",
    "            ]\n",
    "        elif \"gte\" in args.model_name:\n",
    "            sentences = [\n",
    "                row.prompt\n",
    "                + \" \"\n",
    "                + row.A\n",
    "                + \" \"\n",
    "                + row.B\n",
    "                + \" \"\n",
    "                + row.C\n",
    "                + \" \"\n",
    "                + row.D\n",
    "                + \" \"\n",
    "                + row.E\n",
    "            ]\n",
    "\n",
    "        embeddings = torch.Tensor(\n",
    "            model.encode(sentences, show_progress_bar=False, normalize_embeddings=True)\n",
    "        )\n",
    "        embs.append(torch.nn.functional.normalize(embeddings, dim=1))\n",
    "\n",
    "    query_embeddings = torch.Tensor(np.stack(embs)).squeeze(1)\n",
    "\n",
    "    # Create placeholders for top-k matches\n",
    "    all_vals_gpu_0 = torch.full((len(test), TOP_K), -float(\"inf\"), dtype=torch.float16)\n",
    "    all_texts_gpu_0 = [[None] * TOP_K for _ in range(len(all_vals_gpu_0))]\n",
    "\n",
    "    all_vals_gpu_1 = torch.full((len(test), TOP_K), -float(\"inf\"), dtype=torch.float16)\n",
    "    all_texts_gpu_1 = [[None] * TOP_K for _ in range(len(all_vals_gpu_1))]\n",
    "\n",
    "    def load_data(files, device):\n",
    "        for file, file_np in files:\n",
    "            df = pd.read_parquet(file, engine=\"pyarrow\", use_threads=True)\n",
    "            file_embeddings = np.load(file_np)\n",
    "\n",
    "            data_embeddings = torch.Tensor(file_embeddings).to(device).half()\n",
    "            data_embeddings = torch.nn.functional.normalize(data_embeddings, dim=1)\n",
    "\n",
    "            max_inds, max_vals = get_topk(\n",
    "                query_embeddings, data_embeddings, topk=TOP_K, bs=8\n",
    "            )\n",
    "\n",
    "            # loop through all queries (test)\n",
    "            for i in range(len(test)):\n",
    "                # start with highest new val (pos 0) vs worst value already in the toplist (pos topk - 1)\n",
    "                for new in range(TOP_K):\n",
    "                    if device == \"cuda:0\":\n",
    "                        if max_vals[i][new].item() < all_vals_gpu_0[i][TOP_K - 1]:\n",
    "                            break\n",
    "                        for old in range(TOP_K):\n",
    "                            if max_vals[i][new].item() > all_vals_gpu_0[i][old]:\n",
    "                                all_vals_gpu_0[i] = insert_value_at(\n",
    "                                    all_vals_gpu_0[i],\n",
    "                                    value=max_vals[i][new].item(),\n",
    "                                    position=old,\n",
    "                                )\n",
    "                                all_texts_gpu_0[i] = insert_value_at_list(\n",
    "                                    all_texts_gpu_0[i],\n",
    "                                    value=df.iloc[max_inds[i][new].item()].text,\n",
    "                                    position=old,\n",
    "                                )\n",
    "                                break\n",
    "                    else:\n",
    "                        if max_vals[i][new].item() < all_vals_gpu_1[i][TOP_K - 1]:\n",
    "                            break\n",
    "                        for old in range(TOP_K):\n",
    "                            if max_vals[i][new].item() > all_vals_gpu_1[i][old]:\n",
    "                                all_vals_gpu_1[i] = insert_value_at(\n",
    "                                    all_vals_gpu_1[i],\n",
    "                                    value=max_vals[i][new].item(),\n",
    "                                    position=old,\n",
    "                                )\n",
    "                                all_texts_gpu_1[i] = insert_value_at_list(\n",
    "                                    all_texts_gpu_1[i],\n",
    "                                    value=df.iloc[max_inds[i][new].item()].text,\n",
    "                                    position=old,\n",
    "                                )\n",
    "                                break\n",
    "\n",
    "    Parallel(n_jobs=2, backend=\"threading\")(\n",
    "        delayed(load_data)(files[i], f\"cuda:{i}\") for i in range(2)\n",
    "    )\n",
    "    all_vals = torch.hstack([all_vals_gpu_0, all_vals_gpu_1])\n",
    "    val, inds = torch.topk(all_vals.float(), axis=1, k=TOP_K)\n",
    "    all_texts = [\n",
    "        [(t0 + t1)[inner_idx.item()] for inner_idx in idx]\n",
    "        for t0, t1, idx in zip(all_texts_gpu_0, all_texts_gpu_1, inds)\n",
    "    ]\n",
    "\n",
    "    all_texts = [remove_consecutive_duplicates(lst) for lst in all_texts]\n",
    "\n",
    "    test[\"context\"] = [\n",
    "        \"\\n###\\n\".join([x[i] for i in list(range(args.topk))[::-1]]) for x in all_texts\n",
    "    ]\n",
    "\n",
    "    test[\"context_v2\"] = [\n",
    "        \"Context 4: \"\n",
    "        + x[4]\n",
    "        + \"\\n###\\n\"\n",
    "        + \"Context 3: \"\n",
    "        + x[3]\n",
    "        + \"\\n###\\n\"\n",
    "        + \"Context 2: \"\n",
    "        + x[2]\n",
    "        + \"\\n###\\n\"\n",
    "        + \"Context 1: \"\n",
    "        + x[1]\n",
    "        + \"\\n###\\n\"\n",
    "        + \"Context 0: \"\n",
    "        + x[0]\n",
    "        for x in all_texts\n",
    "    ]\n",
    "\n",
    "    print(test[\"context\"].values[0])\n",
    "\n",
    "    test.to_parquet(f\"{args.test_file}.pq\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf32d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T11:37:31.178755Z",
     "iopub.status.busy": "2023-10-16T11:37:31.178473Z",
     "iopub.status.idle": "2023-10-16T11:37:31.183856Z",
     "shell.execute_reply": "2023-10-16T11:37:31.182726Z"
    },
    "papermill": {
     "duration": 0.011924,
     "end_time": "2023-10-16T11:37:31.185577",
     "exception": false,
     "start_time": "2023-10-16T11:37:31.173653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "\n",
    "python get_topk.py --wiki \"cirrus\" --model_name \"e5-large\" --test_file \"test\" --topk 5 --ind 0  &&\n",
    "python get_topk.py --wiki \"new\" --model_name \"gte-large\" --test_file \"test2\" --topk 5 --ind 0  &&\n",
    "\n",
    "wait \n",
    "echo \"All done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94700dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T11:37:31.195098Z",
     "iopub.status.busy": "2023-10-16T11:37:31.194860Z",
     "iopub.status.idle": "2023-10-16T12:09:41.070071Z",
     "shell.execute_reply": "2023-10-16T12:09:41.068907Z"
    },
    "papermill": {
     "duration": 1929.88238,
     "end_time": "2023-10-16T12:09:41.072164",
     "exception": false,
     "start_time": "2023-10-16T11:37:31.189784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sh run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b8706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T12:09:41.084145Z",
     "iopub.status.busy": "2023-10-16T12:09:41.083853Z",
     "iopub.status.idle": "2023-10-16T12:10:38.536605Z",
     "shell.execute_reply": "2023-10-16T12:10:38.535365Z"
    },
    "papermill": {
     "duration": 57.46124,
     "end_time": "2023-10-16T12:10:38.538940",
     "exception": false,
     "start_time": "2023-10-16T12:09:41.077700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/sci-llm-pip-v2/bitsandbytes-0.41.0-py3-none-any.whl\n",
    "!pip install --no-index --find-links=\"/kaggle/input/transformers-main\" /kaggle/input/transformers-main/transformers-4.34.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ebc15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T12:10:38.553511Z",
     "iopub.status.busy": "2023-10-16T12:10:38.553189Z",
     "iopub.status.idle": "2023-10-16T12:10:38.561805Z",
     "shell.execute_reply": "2023-10-16T12:10:38.560824Z"
    },
    "papermill": {
     "duration": 0.018279,
     "end_time": "2023-10-16T12:10:38.563519",
     "exception": false,
     "start_time": "2023-10-16T12:10:38.545240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import itertools\n",
    "\n",
    "def longest_common_prefix(strs):\n",
    "    if not strs:\n",
    "        return \"\"\n",
    "\n",
    "    shortest = min(strs, key=len)\n",
    "\n",
    "    for i, char in enumerate(shortest):\n",
    "        for other in strs:\n",
    "            if other[i] != char:\n",
    "                return shortest[:i]\n",
    "\n",
    "    return shortest\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--device\", type=str, required=True)\n",
    "    ap.add_argument(\"--model_name\", type=str, required=True)\n",
    "    ap.add_argument(\"--quantization\", type=int, required=True)\n",
    "    ap.add_argument(\"--model_type\", type=int, required=True)\n",
    "    ap.add_argument(\"--test_file\", type=str, required=True)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    if args.device == \"auto\":\n",
    "        DEVICE_MAP = \"auto\"\n",
    "        DEVICE = \"cuda:0\"\n",
    "    else:\n",
    "        DEVICE_MAP = {\"\": args.device}\n",
    "        DEVICE = args.device\n",
    "\n",
    "    llm_backbone = args.model_name\n",
    "\n",
    "    test = pd.read_parquet(args.test_file).reset_index(drop=True)\n",
    "\n",
    "    new_obs = []\n",
    "    for idx, row in test.iterrows():\n",
    "        for opt in \"ABCDE\":\n",
    "            new_obs.append(\n",
    "                (row[\"id\"], row[\"context\"], row[\"context_v2\"], row[\"prompt\"], row[opt])\n",
    "            )\n",
    "    df = pd.DataFrame(\n",
    "        new_obs, columns=[\"id\", \"context\", \"context_v2\", \"question\", \"answer\"]\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        llm_backbone,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\",\n",
    "        truncation_side=\"left\",\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.unk_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "        else:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    if args.quantization == 0:\n",
    "        quantization_config = None\n",
    "    elif args.quantization == 1:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True, llm_int8_threshold=0.0\n",
    "        )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_backbone,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=DEVICE_MAP,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "    ).eval()\n",
    "\n",
    "    head_weights = torch.load(llm_backbone + \"/head.pth\", map_location=\"cpu\")\n",
    "    hidden_size = head_weights.shape[1]\n",
    "\n",
    "    head = torch.nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    head.weight.data = head_weights\n",
    "\n",
    "    head.to(DEVICE).eval()\n",
    "\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    progress_bar = tqdm(df.iterrows(), total=len(df))\n",
    "\n",
    "    preds = []\n",
    "    instructions = []\n",
    "    pooled = []\n",
    "    past_key_values = None\n",
    "    for idx, row in progress_bar:\n",
    "        inst = f\"Answer: {row['answer']}\\n###\\nIs this answer correct? \"\n",
    "        instructions.append(inst)\n",
    "\n",
    "        if idx % 5 == 0:\n",
    "            if past_key_values is not None:\n",
    "                del past_key_values\n",
    "\n",
    "            preprompt = f\"{row['context_v2']}\\n###\\nQuestion: {row['question']}\\n###\\n\"\n",
    "            inputs = tokenizer(\n",
    "                preprompt,\n",
    "                return_tensors=\"pt\",\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                padding=\"longest\",\n",
    "                max_length=1024,\n",
    "            )\n",
    "\n",
    "            tok_length = (\n",
    "                inputs[\"input_ids\"].shape[1]\n",
    "                + tokenizer(\n",
    "                    instructions,\n",
    "                    return_tensors=\"pt\",\n",
    "                    add_special_tokens=False,\n",
    "                    truncation=True,\n",
    "                    padding=\"longest\",\n",
    "                    max_length=1024,\n",
    "                )[\"input_ids\"].shape[1]\n",
    "            )\n",
    "\n",
    "            BATCH_SIZE = 5\n",
    "\n",
    "            with torch.no_grad():\n",
    "                past_key_values = list(\n",
    "                    model(input_ids=inputs[\"input_ids\"].to(DEVICE)).past_key_values\n",
    "                )\n",
    "\n",
    "                for idx0 in range(len(past_key_values)):\n",
    "                    past_key_values[idx0] = list(past_key_values[idx0])\n",
    "                    for idx1 in range(len(past_key_values[idx0])):\n",
    "                        past_key_values[idx0][idx1] = past_key_values[idx0][\n",
    "                            idx1\n",
    "                        ].expand(BATCH_SIZE, -1, -1, -1)\n",
    "            del inputs\n",
    "\n",
    "        if (idx + 1) % BATCH_SIZE == 0 or idx == len(df) - 1:\n",
    "            inputs = tokenizer(\n",
    "                instructions,\n",
    "                return_tensors=\"pt\",\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                padding=\"longest\",\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(\n",
    "                    input_ids=inputs[\"input_ids\"].to(DEVICE),\n",
    "                    past_key_values=past_key_values,\n",
    "                ).logits\n",
    "\n",
    "                for jjj in range(len(out)):\n",
    "                    att_idx = inputs[\"attention_mask\"].sum(dim=1)[jjj] - 1\n",
    "                    pooled.append(out[jjj, att_idx, :].float().unsqueeze(0))\n",
    "\n",
    "            instructions = []\n",
    "            del out\n",
    "            del inputs\n",
    "\n",
    "        if (idx + 1) % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                pooled = torch.cat(pooled)\n",
    "\n",
    "                new_poolings = []\n",
    "\n",
    "                indexes = np.arange(0, 5)\n",
    "                for jj in indexes:\n",
    "                    other_embeddings = pooled[\n",
    "                        [jjj for jjj in indexes if jjj != jj]\n",
    "                    ]\n",
    "                    new_poolings.append(\n",
    "                        torch.cat(\n",
    "                            [pooled[jj], torch.mean(other_embeddings, dim=0)]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                new_poolings = torch.stack(new_poolings)\n",
    "\n",
    "                logits = head(new_poolings)\n",
    "                logits = logits[:, 0]\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "\n",
    "                for lg in logits:\n",
    "                    preds.append(lg)\n",
    "\n",
    "            del logits\n",
    "            pooled = []\n",
    "\n",
    "    np.save(f\"scores_{llm_backbone.split('/')[-1]}_{args.test_file[:-3]}\", preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2edd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T12:10:38.575765Z",
     "iopub.status.busy": "2023-10-16T12:10:38.575532Z",
     "iopub.status.idle": "2023-10-16T12:10:38.580584Z",
     "shell.execute_reply": "2023-10-16T12:10:38.579599Z"
    },
    "papermill": {
     "duration": 0.013324,
     "end_time": "2023-10-16T12:10:38.582368",
     "exception": false,
     "start_time": "2023-10-16T12:10:38.569044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "\n",
    "python inference.py --device \"auto\" --model_name \"/kaggle/input/teamhydrogen-white-malamute-prompt-openorca-v2\" --quantization 0 --model_type 1 --test_file \"test.pq\"  &&\n",
    "python inference.py --device \"auto\" --model_name \"/kaggle/input/teamhydrogen-white-malamute-prompt-openorca-v2\" --quantization 0 --model_type 1 --test_file \"test2.pq\"  &&\n",
    "\n",
    "wait \n",
    "echo \"All done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d7263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T12:10:38.595438Z",
     "iopub.status.busy": "2023-10-16T12:10:38.594637Z",
     "iopub.status.idle": "2023-10-16T12:22:33.462522Z",
     "shell.execute_reply": "2023-10-16T12:22:33.461321Z"
    },
    "papermill": {
     "duration": 714.877098,
     "end_time": "2023-10-16T12:22:33.465279",
     "exception": false,
     "start_time": "2023-10-16T12:10:38.588181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sh run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b786934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T12:22:33.555936Z",
     "iopub.status.busy": "2023-10-16T12:22:33.554884Z",
     "iopub.status.idle": "2023-10-16T12:22:33.978965Z",
     "shell.execute_reply": "2023-10-16T12:22:33.978087Z"
    },
    "papermill": {
     "duration": 0.468717,
     "end_time": "2023-10-16T12:22:33.981535",
     "exception": false,
     "start_time": "2023-10-16T12:22:33.512818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from scipy.special import softmax\n",
    "\n",
    "test = pd.read_parquet(\"test.pq\")\n",
    "curr_scores = []\n",
    "\n",
    "for f in glob.glob(\"scores_*.npy\"):\n",
    "    print(f)\n",
    "    a = np.array(np.load(f))\n",
    "    a = softmax(a.reshape(-1,5), axis=1)\n",
    "    a = a.flatten()\n",
    "    print(a.mean(axis=0))\n",
    "    curr_scores.append(a)\n",
    "    os.remove(f)\n",
    "curr_scores = np.array(curr_scores)\n",
    "preds = np.nanmean(curr_scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceccec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T12:22:34.082472Z",
     "iopub.status.busy": "2023-10-16T12:22:34.082101Z",
     "iopub.status.idle": "2023-10-16T12:22:34.098485Z",
     "shell.execute_reply": "2023-10-16T12:22:34.097723Z"
    },
    "papermill": {
     "duration": 0.067802,
     "end_time": "2023-10-16T12:22:34.100701",
     "exception": false,
     "start_time": "2023-10-16T12:22:34.032899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "options = \"ABCDE\"\n",
    "indices = list(range(5))\n",
    "\n",
    "option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "index_to_option = {index: option for option, index in zip(options, indices)}\n",
    "post_proc_preds = []\n",
    "\n",
    "for gr in range(0, len(preds), 5):\n",
    "    pr = preds[gr : gr + 5]\n",
    "    pr = np.argsort(-pr)[:3]\n",
    "    post_proc_preds.append(\" \".join([index_to_option[x] for x in pr]))\n",
    "len(post_proc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95248928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T12:22:34.193373Z",
     "iopub.status.busy": "2023-10-16T12:22:34.193077Z",
     "iopub.status.idle": "2023-10-16T12:22:34.198109Z",
     "shell.execute_reply": "2023-10-16T12:22:34.197360Z"
    },
    "papermill": {
     "duration": 0.05491,
     "end_time": "2023-10-16T12:22:34.201490",
     "exception": false,
     "start_time": "2023-10-16T12:22:34.146580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[\"prediction\"] = post_proc_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278aadbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T12:22:34.293451Z",
     "iopub.status.busy": "2023-10-16T12:22:34.293133Z",
     "iopub.status.idle": "2023-10-16T12:22:34.325097Z",
     "shell.execute_reply": "2023-10-16T12:22:34.324291Z"
    },
    "papermill": {
     "duration": 0.080921,
     "end_time": "2023-10-16T12:22:34.327438",
     "exception": false,
     "start_time": "2023-10-16T12:22:34.246517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CALC_SCORE:\n",
    "\n",
    "    import numpy as np\n",
    "    def precision_at_k(r, k):\n",
    "        \"\"\"Precision at k\"\"\"\n",
    "        assert k <= len(r)\n",
    "        assert k != 0\n",
    "        return sum(int(x) for x in r[:k]) / k\n",
    "\n",
    "    def MAP_at_3(predictions, true_items):\n",
    "        \"\"\"Score is mean average precision at 3\"\"\"\n",
    "        U = len(predictions)\n",
    "        map_at_3 = 0.0\n",
    "        for u in range(U):\n",
    "            user_preds = predictions[u]\n",
    "            user_true = true_items[u]\n",
    "            user_results = [1 if item == user_true else 0 for item in user_preds]\n",
    "            for k in range(min(len(user_preds), 3)):\n",
    "                map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n",
    "        return map_at_3 / U\n",
    "\n",
    "    maps = []\n",
    "    for idx, row in test.iterrows():\n",
    "\n",
    "        tl = row[\"answer\"]\n",
    "        pr = row[\"prediction\"].split(\" \")\n",
    "\n",
    "        map = MAP_at_3([pr], [tl])\n",
    "        maps.append(map)\n",
    "\n",
    "    print(np.mean(maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b372bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T12:22:34.403880Z",
     "iopub.status.busy": "2023-10-16T12:22:34.403590Z",
     "iopub.status.idle": "2023-10-16T12:22:34.429503Z",
     "shell.execute_reply": "2023-10-16T12:22:34.428460Z"
    },
    "papermill": {
     "duration": 0.055429,
     "end_time": "2023-10-16T12:22:34.431288",
     "exception": false,
     "start_time": "2023-10-16T12:22:34.375859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[[\"id\", \"prediction\"]].to_csv(\"submission.csv\", index=False)\n",
    "pd.read_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2759.866079,
   "end_time": "2023-10-16T12:22:35.628057",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-16T11:36:35.761978",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
