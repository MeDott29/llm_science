{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Construction Across Multiple Domains and Difficulties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "class MultipleChoiceQuestion(BaseModel):\n",
    "    item_id: str\n",
    "    stem: str  # The actual question text\n",
    "    options: List[str]  # The list of answer choices\n",
    "    key_index: int  # Index of the correct answer in the options list\n",
    "    difficulty_level: str = None  # Optional difficulty level of the question\n",
    "    skill_domain: str = None  # Optional specific skill or knowledge domain\n",
    "\n",
    "# Read the CSV file using Pandas\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "# We don't have difficulty_level and skill_domain provided in the CSV, so we'll skip these.\n",
    "# If available, they could be added similarly.\n",
    "\n",
    "# Transform the data into MultipleChoiceQuestion instances\n",
    "mcq_list = []\n",
    "for _, row in df.iterrows():\n",
    "    options = [row['A'], row['B'], row['C'], row['D'], row['E']]\n",
    "    answer_letter = row['answer'].upper()  # Ensure it's uppercase\n",
    "    key_index = ord(answer_letter) - ord('A')  # Convert letter to index\n",
    "    try:\n",
    "        mcq_item = MultipleChoiceQuestion(\n",
    "            item_id=str(row['id']),\n",
    "            stem=row['prompt'],\n",
    "            options=options,\n",
    "            key_index=key_index,\n",
    "        )\n",
    "        mcq_list.append(mcq_item)\n",
    "    except ValidationError as e:\n",
    "        print(f\"Error processing row with id {row['id']}: {e}\")\n",
    "\n",
    "# mcq_list now contains a list of MultipleChoiceQuestion instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultipleChoiceQuestion(item_id='19', stem='What is reciprocal length or inverse length?', options=['Reciprocal length or inverse length is a quantity or measurement used in physics and chemistry. It is the reciprocal of time, and common units used for this measurement include the reciprocal second or inverse second (symbol: s−1), the reciprocal minute or inverse minute (symbol: min−1).', 'Reciprocal length or inverse length is a quantity or measurement used in geography and geology. It is the reciprocal of area, and common units used for this measurement include the reciprocal square metre or inverse square metre (symbol: m−2), the reciprocal square kilometre or inverse square kilometre (symbol: km−2).', 'Reciprocal length or inverse length is a quantity or measurement used in biology and medicine. It is the reciprocal of mass, and common units used for this measurement include the reciprocal gram or inverse gram (symbol: g−1), the reciprocal kilogram or inverse kilogram (symbol: kg−1).', 'Reciprocal length or inverse length is a quantity or measurement used in economics and finance. It is the reciprocal of interest rate, and common units used for this measurement include the reciprocal percent or inverse percent (symbol: %−1), the reciprocal basis point or inverse basis point (symbol: bp−1).', 'Reciprocal length or inverse length is a quantity or measurement used in several branches of science and mathematics. It is the reciprocal of length, and common units used for this measurement include the reciprocal metre or inverse metre (symbol: m−1), the reciprocal centimetre or inverse centimetre (symbol: cm−1).'], key_index=4, difficulty_level=None, skill_domain=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq_list[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes a multiple choice question easy for a human? For a language model? Medium? Hard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"question\": {\n",
    "        \"id\": \"unique_identifier\",\n",
    "        \"text\": \"What is the capital of France?\",\n",
    "        \"domain\": \"Geography\",\n",
    "        \"subdomain\": \"European Capitals\",\n",
    "        \"difficulty_level\": \"Easy\",\n",
    "        \"tags\": [\"Capital Cities\", \"Europe\", \"World Geography\"]\n",
    "    },\n",
    "    \"options\": [\n",
    "        {\"id\": \"A\", \"text\": \"Paris\", \"is_correct\": True},\n",
    "        {\"id\": \"B\", \"text\": \"Berlin\", \"is_correct\": False},\n",
    "        {\"id\": \"C\", \"text\": \"Madrid\", \"is_correct\": False},\n",
    "        {\"id\": \"D\", \"text\": \"Rome\", \"is_correct\": False}\n",
    "    ],\n",
    "    \"explanation\": {\n",
    "        \"text\": \"Paris is the capital of France. Berlin is the capital of Germany, Madrid is the capital of Spain, and Rome is the capital of Italy.\",\n",
    "        \"references\": [\n",
    "            {\"source\": \"World Atlas\", \"url\": \"https://www.worldatlas.com/europe/france\"}\n",
    "            # Other references can be included as necessary\n",
    "        ]\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"author\": \"QuizMaster\",\n",
    "        \"creation_date\": \"2023-01-01\",\n",
    "        \"last_update\": \"2023-01-05\",\n",
    "        \"usage\": {\"times_used\": 100, \"average_score\": 0.85}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, HttpUrl, validator\n",
    "\n",
    "class Option(BaseModel):\n",
    "    id: str\n",
    "    text: str\n",
    "    is_correct: bool\n",
    "\n",
    "class ExplanationReference(BaseModel):\n",
    "    source: str\n",
    "    url: HttpUrl\n",
    "\n",
    "class Explanation(BaseModel):\n",
    "    text: str\n",
    "    references: List[ExplanationReference]\n",
    "\n",
    "class UsageMetadata(BaseModel):\n",
    "    times_used: int\n",
    "    average_score: float\n",
    "\n",
    "class QuestionMetadata(BaseModel):\n",
    "    author: str\n",
    "    creation_date: str  # Could be a datetime field if you require a datetime object\n",
    "    last_update: str\n",
    "    usage: UsageMetadata\n",
    "\n",
    "class MultipleChoiceQuestion(BaseModel):\n",
    "    id: str\n",
    "    text: str\n",
    "    domain: str\n",
    "    subdomain: Optional[str]\n",
    "    difficulty_level: str\n",
    "    tags: List[str]\n",
    "    options: List[Option]\n",
    "    explanation: Explanation\n",
    "    metadata: QuestionMetadata\n",
    "\n",
    "    @validator('options')\n",
    "    def validate_options(cls, options):\n",
    "        correct_options = [option for option in options if option.is_correct]\n",
    "        if len(correct_options) != 1:\n",
    "            raise ValueError('There must be exactly one correct option')\n",
    "        return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21712/1511559684.py:51: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @validator('incorrect_options', pre=True)\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, HttpUrl, validator\n",
    "from datetime import date\n",
    "\n",
    "# Base models\n",
    "class BaseOption(BaseModel):\n",
    "    id: str\n",
    "    text: str\n",
    "\n",
    "class CorrectOption(BaseOption):\n",
    "    is_correct: bool = True\n",
    "\n",
    "class IncorrectOption(BaseOption):\n",
    "    is_correct: bool = False\n",
    "\n",
    "class Reference(BaseModel):\n",
    "    source: str\n",
    "    url: HttpUrl\n",
    "\n",
    "class Explanation(BaseModel):\n",
    "    text: str\n",
    "    references: List[Reference]\n",
    "\n",
    "class UsageMetadata(BaseModel):\n",
    "    times_used: int\n",
    "    average_score: float\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    author: str\n",
    "    creation_date: date\n",
    "    last_update: date\n",
    "    usage: UsageMetadata\n",
    "\n",
    "# Question model with inheritance\n",
    "class MultipleChoiceQuestion(BaseModel):\n",
    "    id: str\n",
    "    text: str\n",
    "    domain: str\n",
    "    subdomain: str = None\n",
    "    difficulty_level: str\n",
    "    tags: List[str]\n",
    "    correct_option: CorrectOption\n",
    "    incorrect_options: List[IncorrectOption]\n",
    "    explanation: Explanation\n",
    "    metadata: Metadata\n",
    "\n",
    "    @property\n",
    "    def options(self):\n",
    "        return [self.correct_option] + self.incorrect_options\n",
    "\n",
    "    @validator('incorrect_options', pre=True)\n",
    "    def validate_correct_option(cls, incorrect_options):\n",
    "        if not any(o.is_correct for o in incorrect_options):\n",
    "            raise ValueError('There must be exactly one correct option')\n",
    "        return incorrect_options\n",
    "    \n",
    "    class Config:\n",
    "        validate_assignment = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "question = MultipleChoiceQuestion(\n",
    "    id=\"q1\",\n",
    "    text=\"What is the capital of France?\",\n",
    "    domain=\"Geography\",\n",
    "    difficulty_level=\"Easy\",\n",
    "    tags=[\"Capital Cities\", \"Europe\", \"World Geography\"],\n",
    "    correct_option=CorrectOption(\n",
    "        id=\"A\",\n",
    "        text=\"Paris\",\n",
    "    ),\n",
    "    incorrect_options=[\n",
    "        IncorrectOption(id=\"B\", text=\"Berlin\"),\n",
    "        IncorrectOption(id=\"C\", text=\"Madrid\"),\n",
    "        IncorrectOption(id=\"D\", text=\"Rome\"),\n",
    "    ],\n",
    "    explanation=Explanation(\n",
    "        text=\"Paris is the capital of France. Berlin is the capital of Germany, Madrid is the capital of Spain, and Rome is the capital of Italy.\",\n",
    "        references=[\n",
    "            Reference(\n",
    "                source=\"World Atlas\",\n",
    "                url=\"https://www.worldatlas.com/europe/france\"\n",
    "            ),\n",
    "            # Other references can be included as necessary\n",
    "        ]\n",
    "    ),\n",
    "    metadata=Metadata(\n",
    "        author=\"QuizMaster\",\n",
    "        creation_date=date(2023, 1, 1),\n",
    "        last_update=date(2023, 1, 5),\n",
    "        usage=UsageMetadata(\n",
    "            times_used=100,\n",
    "            average_score=0.85\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Printing the question to verify the structure\n",
    "print(question.json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
