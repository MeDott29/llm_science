{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roughing in Architecture\n",
    "#### Employ Training Data for Outlining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import instructor\n",
    "from instructor import patch\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict\n",
    "\n",
    "class WikipediaAPICall(BaseModel):\n",
    "    action: str\n",
    "    format: str\n",
    "    prop: str\n",
    "    titles: str\n",
    "    choices: str\n",
    "\n",
    "client = patch(OpenAI())\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "row = df.iloc[0]\n",
    "\n",
    "test_item = row['prompt']  + ' ' + row['A'] + ' ' + row['B'] + ' ' + row['C'] + ' ' + row['D'] + ' ' + row['E'] + ' ' + row['answer']\n",
    "\n",
    "wiki_api_call = WikipediaAPICall(\n",
    "    action=\"query\",\n",
    "    format=\"json\",\n",
    "    prop=\"extracts\",\n",
    "    titles=test_item,\n",
    "    choices=\"content\"\n",
    ")\n",
    "\n",
    "client = instructor.patch(client)\n",
    "\n",
    "class SearchTerms(BaseModel):\n",
    "    first: str\n",
    "    second: str\n",
    "    third: str\n",
    "    \n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Read the MCQ and give the top three terms you would use to search for an answer: {test_item}\",\n",
    "        },\n",
    "    ],\n",
    "    response_model=SearchTerms\n",
    ")\n",
    "resp\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_string(s):\n",
    "  # Remove everything inside parentheses\n",
    "  s = re.sub(r'\\([^()]*\\)', '', s)\n",
    "  \n",
    "  # Remove all non-alphabetical characters except spaces\n",
    "  s = re.sub(r'[^a-zA-Z ]', '', s)\n",
    "  \n",
    "  # Remove spaces from the end of the string\n",
    "  s = s.rstrip()\n",
    "  \n",
    "  return s\n",
    "search_term = clean_string(resp.first)\n",
    "print(search_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def print_first_two_wikipedia_pages_content(search_query):\n",
    "    S = requests.Session()\n",
    "\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    SEARCHQUERY = search_query\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"opensearch\",\n",
    "        \"format\": \"json\",\n",
    "        \"search\": SEARCHQUERY,\n",
    "        \"limit\": 2,\n",
    "    }\n",
    "\n",
    "    response = S.get(url=URL, params=PARAMS)\n",
    "    data = response.json()\n",
    "\n",
    "    page_titles = data[1]\n",
    "\n",
    "    for title in page_titles:\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "        }\n",
    "\n",
    "        response = S.get(url=URL, params=PARAMS)\n",
    "        data = response.json()\n",
    "\n",
    "        pages = data[\"query\"][\"pages\"]\n",
    "\n",
    "        for k, v in pages.items():\n",
    "            print(v[\"extract\"])\n",
    "\n",
    "print_first_two_wikipedia_pages_content(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_first_two_wikipedia_pages_content(search_query):\n",
    "    S = requests.Session()\n",
    "\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    SEARCHQUERY = search_query\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"opensearch\",\n",
    "        \"format\": \"json\",\n",
    "        \"search\": SEARCHQUERY,\n",
    "        \"limit\": 2,\n",
    "    }\n",
    "\n",
    "    response = S.get(url=URL, params=PARAMS)\n",
    "    data = response.json()\n",
    "\n",
    "    page_titles = data[1]\n",
    "\n",
    "    content = \"\"\n",
    "    for title in page_titles:\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "        }\n",
    "\n",
    "        response = S.get(url=URL, params=PARAMS)\n",
    "        data = response.json()\n",
    "\n",
    "        pages = data[\"query\"][\"pages\"]\n",
    "\n",
    "        for k, v in pages.items():\n",
    "            content += v[\"extract\"]\n",
    "    \n",
    "    return content\n",
    "def find_terms_in_same_paragraph(text, term1, term2):\n",
    "  paragraphs = re.split(r'\\s*\\n\\s*', text)\n",
    "  matching_paragraphs = []\n",
    "  for paragraph in paragraphs:\n",
    "      if re.search(fr'\\b{term1}\\b', paragraph, re.IGNORECASE) and re.search(fr'\\b{term2}\\b', paragraph, re.IGNORECASE):\n",
    "          matching_paragraphs.append(paragraph)\n",
    "  if not matching_paragraphs:\n",
    "      return \"No paragraph found containing the specified terms.\"\n",
    "  return matching_paragraphs\n",
    "\n",
    "\n",
    "\n",
    "text = print_first_two_wikipedia_pages_content(search_term)\n",
    "\n",
    "paragraphs = find_terms_in_same_paragraph(text, resp.second, resp.third)\n",
    "for i, paragraph in enumerate(paragraphs):\n",
    "    print(f\"Paragraph {i+1}:\\n{paragraph}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
